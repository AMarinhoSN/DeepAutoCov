from load_data import *
from get_lineage_class import *
from map_lineage_to_finalclass import *
from optparse import OptionParser
from nweek_req_nseq import *
from valid_lineages import *
from weeks_retraining import *
from datetime import *
from Autoencoder_training_GPU import *
import logging
from sklearn.model_selection import ParameterGrid
from sensitivity import *
from falsepositive import *
from fraction_mail import *
from barplot_laboratory import *
from discovery import *
from filter_dataset import *
from test_normality_error import *
import gc
from PRC_curve import *
from model_dl import *
from Autoencoder_training import *
from PRC_Graphic_curve import *
from Best_worse import *
from plot_smooth import *
from kmers_error import *


def main(options):
    # memory control
    gc.enable()
    # use GPU
    strategy = tf.distribute.MirroredStrategy()

    # define a list that we will use during the code
    prediction_lineages = []  # A list to store predictions for each lineage and for each week.
    summary_lineages = [] # A list to store a summary for each lineage and for each week.
    number_of_features = []  # Number of features
    results_fine_tune = []
    fractions_100 = []  # A list to store predictions for the top 100 sequences with higher mean squared error (MSE).
    mse_prc_curve = []  # A list to store Mean Squared Error (MSE) values for generating the Precision-Recall Curve (PRC).
    true_class_prc_curve = [] # A list to store Mean Squared Error (MSE) values for generating the Precision-Recall Curve (PRC).
    info_PCR = []  
    ind_prc = 0 # Index

    
    dir_week =str(options.path_drive) # Path dataset generated by Data_Filtration_kmers.py
    
    metadata = pd.read_csv(str(options.csv_path)) # Read metadata filtered file generated by Data_Filtration_kmers.py

    # columns in metadata filtered file
    col_class_lineage = 'Pango.lineage'
    col_submission_date = 'Collection.date'
    col_lineage_id = 'Accession.ID'

    
    valid_lineage,valid_lineage_prc,dictionary_lineage_weekly=valid_lineages() # Function that return the Future Dominant Lineages (FDLs) of Interest.

    metadata[col_class_lineage] = metadata[col_class_lineage].apply(lambda x: 'unknown' if x not in valid_lineage else x) # Replacement of non-FDLs by unknown.

    # week of retraining
    retraining_week, retraining_week_false_positive=weeks_retrain() # Return retraining weeks.

    # K-mers
    header = pd.read_csv(str(options.kmers), nrows=1)
    features = header.columns[1:].tolist()  # k-mers
    print('-----------------------------------------------------------------------------------')
    print('k-mers : ' + str(len(features)))
    print('-----------------------------------------------------------------------------------')

    # Saving outputs
    path_saving_file=str(options.path_save)

    # lineage of interest
    lineage_of_interest = metadata[col_class_lineage].unique().tolist()
    lineage_of_interest.remove('unknown')

    # logging
    logging.basicConfig(level=logging.INFO,
                        format='%(asctime)s %(message)s',
                        datefmt='%Y-%m-%d %H:%M:%S',
                        handlers=[
                            # logging.FileHandler('/mnt/resources/2022_04/2022_04/'+'run_main_oneclass_retrain_tmp.log', 'w+'),
                            logging.FileHandler(path_saving_file + '/Autoencode_performance.log', 'w+'),
                            logging.StreamHandler()
                        ])

    starting_week = 1 # First week of simulation.

    # Loading first training set.
    df_trainstep_1, train_w_list = load_data(dir_week, [starting_week]) # First training set.
    train_step1 = df_trainstep_1.iloc[:, 1:len(df_trainstep_1.columns)].to_numpy()

    # Filter the features of models.
    sum_train = np.sum(train_step1, axis=0) # Sum the features by column.
    keepFeature=sum_train/len(train_step1)
    i_no_zero = np.where(keepFeature >= options.rate_mantain)[0] # Find the most representative features in the dataset.

    print('---------------------------------------------------------------------------------------------------------------------------------------------------------')
    print('features :' + str((len(i_no_zero))))
    print('---------------------------------------------------------------------------------------------------------------------------------------------------------')

    # Training set
    y_train_initial = metadata[metadata[col_lineage_id].isin(df_trainstep_1.iloc[:, 0].tolist())][col_class_lineage] # Elements of training set.
    y_train_class = map_lineage_to_finalclass(y_train_initial.tolist(), lineage_of_interest) # Class of training set.
    counter_i = Counter(y_train_initial)

    # Filter out unrepresentative features
    train_step_total = train_step1
    train = train_step1[:, i_no_zero] # Select the representative features.
    lineages_train=np.array(y_train_initial.tolist()) # Type of lineages.

    tf.random.set_seed(10)
    # Creation of Autoencoder models
    # Parameters of autoencoder
    nb_epoch = options.number_epoch
    batch_size = options.batch_size
    input_dim =train.shape[1]
    encoding_dim = options.encoding_dim
    hidden_dim_1 = int(encoding_dim / 2)
    hidden_dim_2=int(hidden_dim_1/2)
    hidden_dim_3=int(hidden_dim_2/2)
    hidden_dim_4=int(hidden_dim_3/2)
    hidden_dim_5=int(hidden_dim_4/2)
    reduction_factor = options.red_factor

    p_grid = {'nb_epoch':[nb_epoch],'batch_size':[batch_size],'input_dim':[input_dim],'encoding_dim':[encoding_dim],'hidden_dim_1':[int(encoding_dim / 2)],'hidden_dim_2':[hidden_dim_2],'hidden_dim_3':[hidden_dim_3],'hidden_dim_4':[hidden_dim_4],'hidden_dim_5':[hidden_dim_5],'Reduction_factor':[reduction_factor]}
    all_combo = list(ParameterGrid(p_grid))

    with strategy.scope():
        autoencoder=model(input_dim,encoding_dim,hidden_dim_1,hidden_dim_2,hidden_dim_3,hidden_dim_4,hidden_dim_5,reduction_factor,path_saving_file)
    for combo in all_combo[0:1]:
        combo
        logging.info("---> Autoencoder - Param: " + str(combo))
        y_test_dict_variant_type = {}
        y_test_dict_finalclass = {}
        y_test_dict_predictedclass = {}
        #train = train_step1.copy()
        history = autoencoder_training_GPU(autoencoder,train, train,nb_epoch,batch_size) # Train the model for the first time.
        print('Trained the model : ')
        print(history)
        info, mse_tr = test_normality(autoencoder, train) #  Compute the MSE in the training set. This is important to define the threshold for the anomaly detetction.
        # Start the simulation
        for week in range(1, 159):
            if week in retraining_week:
                logging.info('----> RETRAINING <-----')
                ind_prc = ind_prc + 1 # Counter
                threshold_retraining = threshold_fixed # Threshold to define if a sequence is anomaly or not
                min_prob = min(mse_prc_curve) # min value of mse
                max_prob = max(mse_prc_curve) # max value of mse
                print('--------PRC--------')
                info_pcr = evaluate_pcr(true_class_prc_curve, mse_prc_curve, threshold_retraining, min_prob, max_prob) # Compute the PRC curve
                info_PCR.append(info_pcr)

                # Creation of new training set to train the model.
                # Filter out unrepresentative features
                train_model_value = train_step_total
                classes=lineages_train
                sum_train = np.sum(train_model_value, axis=0)
                keepFeature = sum_train / len(train_model_value)
                i_no_zero = np.where(keepFeature > options.rate_mantain)[0]

                number_feature = len(i_no_zero)
                print('---------------------------------------------------------------------------------------------------------------------------------------------------------')
                print('features defined :' + str((len(i_no_zero))))
                print('---------------------------------------------------------------------------------------------------------------------------------------------------------')

                train_model_value = train_model_value[:, i_no_zero] # Filter training set with the features of model.
                index_raw = find_index_lineages_for_week(classes, week, dictionary_lineage_weekly) # return index to create the new training set
                train_model_value=train_model_value[index_raw,:]
                np.random.shuffle(train_model_value)
                number_of_features.append(number_feature) # Store the number of features in the retraining week.

                batch_size=512
                input_dim =train_model_value.shape[1]
                with strategy.scope():
                    autoencoder=model(input_dim, encoding_dim, hidden_dim_1, hidden_dim_2, hidden_dim_3, hidden_dim_4, hidden_dim_5,
                          reduction_factor, path_saving_file) # Creation of the AutoEncoder model.
                history = autoencoder_training_GPU(autoencoder, train_model_value, train_model_value, nb_epoch, batch_size) # retraining the model.

                print('Trained the neural network : ')
                print(history)

                info,mse_tr = test_normality(autoencoder, train_model_value) #  Compute the MSE in the training set. This is important to define the threshold for the anomaly detetction.
                train_model_value = []
                classes=[]
                mse_prc_curve = []
                true_class_prc_curve = []
            logging.info("# Week " + str(starting_week + week))
            print("# Week " + str(starting_week + week))

            # Loading test set
            # Download test set from the folder created in the script "Data_Filtration_kmers"
            df_teststep_i, test_w_list = load_data(dir_week, [starting_week + week]) # Loading test set.
            test_step_i = df_teststep_i.iloc[:, 1:len(df_teststep_i.columns)].to_numpy()
            id_identifier = df_teststep_i.iloc[:, 0].to_list() # identifier of sequences.
            test_step_total = test_step_i
            test_step_i = test_step_i[:, i_no_zero] # feature selections.
            y_test_step_i = get_lineage_class(metadata, df_teststep_i.iloc[:, 0].tolist())
            lineages_l=metadata[metadata[col_lineage_id].isin(df_teststep_i.iloc[:, 0].tolist())][col_class_lineage] # Type of lineages in the week of simulation.
            lineages_test=np.array(lineages_l.tolist()) # Lineages in the week of simulation [array].
            y_test_dict_variant_type[starting_week + week] = y_test_step_i
            y_test_fclass_i = map_lineage_to_finalclass(y_test_step_i, lineage_of_interest) # Return the class of test set.
            y_test_fclass_i_prc = map_lineage_to_finalclass(y_test_step_i, valid_lineage_prc[ind_prc]) # Return the class of test set for the function PRC curve.
            i_voc = np.where(np.array(y_test_fclass_i) == -1)[0]
            y_test_dict_finalclass[starting_week + week] = y_test_fclass_i
            lineage_dict = Counter(y_test_step_i)
            test_x_predictions = autoencoder.predict(test_step_i) # Prediction of the model.

            # Threshold
            mse = np.mean(np.power(test_step_i - test_x_predictions, 2), axis=1) # Compute the Mean Squared Error (MSE) between the predictions and the test set.
            mse_prc=list(mse) # mse for the PRC function.
            mse_prc_curve += mse_prc # Stores mse values for the PRC function.
            true_class_prc_curve += y_test_fclass_i_prc  # True class for PRC function
            error_df = pd.DataFrame({'Reconstruction_error': mse})
            threshold_fixed = np.mean(mse_tr) + 1.5 * np.std(mse_tr) # Threshold for anomaly detection.

            print('the threshold is : ' + str(threshold_fixed))
            y_test_i_predict = [-1 if e >= threshold_fixed else 1 for e in error_df.Reconstruction_error.values]
            y_test_i_predict = np.array(y_test_i_predict)

            i_inlier = np.where(y_test_i_predict == 1)[
                0]  

            # selection the first 100 sequences with highest mse.
            TP_100, FP_100, N_100 = Top100(list(mse), y_test_step_i, week, threshold_fixed, 100)
            fractions_100.append([TP_100, FP_100, N_100])
            graphic_fraction(fractions_100, 100, path_saving_file)

            # The k-mers importance
            features_no_zero = [features[i] for i in i_no_zero] # features of model 
            selection_kmers(test_x_predictions, test_step_i, features_no_zero, y_test_i_predict, id_identifier,'Summary_'+str(starting_week+week)+'.csv') # identifies kmers that have not been reproduced correctly.

            # Training set construction
            train_step_total=np.concatenate((train_step_total, test_step_total)) # Store sequences for the next training
            #train_with_class_completo=np.concatenate((train_with_class_completo, test_with_class_completo))
            lineages_train=np.concatenate((lineages_train, lineages_test)) # Store the types of lineages for the next training session.
            y_test_dict_predictedclass[starting_week + week] = y_test_i_predict
            y_test_voc_predict = np.array(y_test_i_predict)[i_voc]

            logging.info("Number of lineage in week:" + str(test_step_i.shape[0]))
            print("Number of lineage in week:" + str(test_step_i.shape[0]))
            logging.info("Number of lineage of concern in week:" + str(len([x for x in y_test_fclass_i if x == -1])))
            print("Number of lineage of concern in week:" + str(len([x for x in y_test_fclass_i if x == -1])))
            logging.info("Distribution of lineage of concern:" + str(Counter(y_test_step_i)))
            print("Distribution of lineage of concern:" + str(Counter(y_test_step_i)))
            logging.info("Number of lineage predicted as anomalty:" + str(
            len([x for x in y_test_dict_predictedclass[starting_week + week] if x == -1])))
            print("Number of lineage predicted as anomalty:" + str(
                len([x for x in y_test_dict_predictedclass[starting_week + week] if x == -1])))
            acc_voc = len([x for x in y_test_voc_predict if x == -1])
            logging.info("Number of lineages of concern predicted as anomalty:" + str(acc_voc))
            print("Number of lineages of concern predicted as anomalty:" + str(acc_voc))

            for k in lineage_dict.keys():
                i_k = np.where(np.array(y_test_step_i) == k)[0]
                logging.info('Number of ' + k + ' lineage:' + str(len(i_k)) + '; predicted anomalty=' + str(
                    len([x for x in y_test_i_predict[i_k] if x == -1])))
                print('Number of ' + k + ' lineage:' + str(len(i_k)) + '; predicted anomalty=' + str(
                    len([x for x in y_test_i_predict[i_k] if x == -1])))
                h = len([x for x in y_test_i_predict[i_k] if x == -1])
                partial_summary = [k, h, week] # The list contains : [Name of lineage, Number of lineage sequences predicted as anomalies, week of simulation].
                prediction_lineages.append(partial_summary) # Store the partial summary
                complete_summary_lineages = [k, len(i_k), h, week]  # The list contains : [Name of lineage,Total number of lineage sequences in the week of simulation,Number of lineage sequences predicted as anomalies, week of simulation].
                summary_lineages.append(complete_summary_lineages) # Store complete summary.
                falsepositive(summary_lineages, retraining_week_false_positive, path_saving_file) # Function to compute the precision graph and the number of false positive.

        # saving results for this comb of param of the oneclass_svm
        results = {'y_test_variant_type': y_test_dict_variant_type,
               'y_test_final_class': y_test_dict_finalclass,
               'y_test_predicted_class': y_test_dict_predictedclass}
    results_fine_tune.append(results)

    print('---------------------------------Vector of information-------------------------------------')
    print(prediction_lineages)
    print(summary_lineages)
    print('---------------------------------Fractions top 100---------------------------------------------------------------------------')
    print(fractions_100)
    print('----------------------------------------------------------------------------------------------------------------------------')

    Precision,Recall,info= compute_prc(info_PCR, path_saving_file)

    print('---------------------------------Precision---------------------------------------------------------------------------')
    print(Precision)
    print('----------------------------------------------------------------------------------------------------------------------------')

    print('---------------------------------Recall---------------------------------------------------------------------------')
    print(Recall)
    print('----------------------------------------------------------------------------------------------------------------------------')

    # THE BEST AND WORST
    best_worst(path_saving_file)

    # FALSE POSITIVE RATE  + boxplot
    FP_RATE_FINAL, Week_final, TN_FINAL, TP_FINAL, FP_FINAL, FN_FINAL = falsepositive(summary_lineages,
                                                                                            retraining_week,
                                                                                            path_saving_file)
    plot_sma(FP_RATE_FINAL, 4, path_saving_file)



    # Others Graphs
    y_true_model0 = results_fine_tune[0]['y_test_final_class']
    y_predict_model0 = results_fine_tune[0]['y_test_predicted_class']

    fp_list = []
    n_list = []
    fn_list = []
    n_outlier_list = []

    for k in y_true_model0.keys():
        yt = np.array(y_true_model0[k])
        yp = np.array(y_predict_model0[k])

        i_inlier = np.where(yt == 1)[0]
        n_fp = len(np.where(yp[i_inlier] == -1)[0])

        fp_list.append(n_fp)
        n_list.append(len(i_inlier))

        i_outlier = np.where(yt == -1)[0]
        n_fn = len(np.where(yp[i_outlier] == 1)[0])
        fn_list.append(n_fn)
        n_outlier_list.append(len(i_outlier))

    tn_list = []
    tp_list = []

    prec_list = []
    recall_list = []
    spec_list = []
    f1_list = []
    for i in range(len(fp_list)):
        tp = n_outlier_list[i] - fn_list[i]
        tn = n_list[i] - fp_list[i]
        tn_list.append(tn)
        tp_list.append(tp)
        if tp + fp_list[i] != 0:
            prec = tp / (tp + fp_list[i])
        else:
            prec = 0

        if tp + fn_list[i] != 0:
            rec = tp / (tp + fn_list[i])
        else:
            rec = 0

        if tn + fp_list[i] != 0:
            spec = tn / (tn + fp_list[i])
        else:
            spec = 0

        if prec + rec != 0:
            f1 = 2 * prec * rec / (prec + rec)
        else:
            f1 = 0
        f1_list.append(f1)
        spec_list.append(spec)
        prec_list.append(prec)
        recall_list.append(rec)

    df_conf = pd.DataFrame()
    df_conf['TN'] = tn_list
    df_conf['FP'] = fp_list
    df_conf['FN'] = fn_list
    df_conf['TP'] = tp_list
    df_conf['Precision'] = prec_list
    df_conf['Recall'] = recall_list
    df_conf['F1'] = f1_list
    df_conf['Specificity'] = spec_list

    # df_conf.to_csv('/mnt/resources/2022_04/2022_04/conf_mat_over_time.tsv', sep='\t', index=None)
    df_conf.to_csv(path_saving_file+'/conf_mat_over_time.tsv', sep='\t', index=None)

    plt.figure(2)
    plt.bar(retraining_week, number_of_features)
    plt.ylabel("No. of features")
    plt.xlabel("Retraining week")
    plt.title("No. of features during retraining")
    plt.savefig(path_saving_file+'/number_of_features.png', dpi=350)

    x = np.arange(len(fp_list))
    fig, ax = plt.subplots(figsize=(32, 14))
    plt.rcParams.update({'font.size': 18})
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    r = plt.bar(x, prec_list, width=0.35, alpha=0.8, color='#a32b15')
    # plt.bar_label(r, rotation=0, fontsize=16)
    plt.yticks(fontsize=25)
    plt.xticks(x, labels=[str(y + 2) for y in x], rotation=45, fontsize=20)
    plt.xlabel('Week', fontsize=25)

    plt.title('Precision in time  from 2019-12 to 2023-02', fontsize=25)
    plt.savefig(path_saving_file+'/precision_in_time.png', dpi=350)

    fig, ax = plt.subplots(figsize=(32, 14))
    plt.rcParams.update({'font.size': 18})
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    r = plt.bar(x, recall_list, width=0.35, alpha=0.8, color='#a32b15')
    # plt.bar_label(r, rotation=0, fontsize=16)
    plt.yticks(fontsize=25)
    plt.xticks(x, labels=[str(y + 2) for y in x], rotation=45, fontsize=20)
    plt.xlabel('Week', fontsize=25)

    plt.title('Recall in time  from 2019-12 to 2023-02', fontsize=25)
    plt.savefig(path_saving_file+'/recall_in_time.png', dpi=350)

    fig, ax = plt.subplots(figsize=(32, 14))
    plt.rcParams.update({'font.size': 18})
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    r = plt.bar(x, f1_list, width=0.35, alpha=0.8, color='#a32b15')
    # plt.bar_label(r, rotation=0, fontsize=16)
    plt.yticks(fontsize=25)
    plt.xticks(x, labels=[str(y + 2) for y in x], rotation=45, fontsize=20)
    plt.xlabel('Week', fontsize=25)

    plt.title('F1 in time  from 2019-12 to 2023-02', fontsize=25)
    plt.savefig(path_saving_file+'/f1_in_time.png', dpi=350)

    x = np.arange(len(fp_list))

    fig, ax = plt.subplots(figsize=(32, 14))
    # plt.rcParams.update({'font.size':18})
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    r = plt.bar(x, fp_list, width=0.35, alpha=0.8, color='#a32b15')
    #new_datalab = [str(x) for x in r.datavalues]
    #for i in range(len(new_datalab)):
        #if i % 2 != 0:
            #new_datalab[i] = ''
    # plt.bar_label(r, new_datalab, fontsize=25, padding=2)
    plt.yticks(fontsize=30)
    newlab_x = []
    for i in range(len(x)):
        if i % 5 == 0:
            newlab_x.append(str(x[i] + 2))
        else:
            newlab_x.append('')

    # plt.xticks(x, labels=[str(y+2) for y in x], rotation=45, fontsize=20)
    plt.xticks(x, labels=newlab_x, rotation=0, fontsize=28)
    plt.xlabel('Week', fontsize=25)
    plt.grid(axis='y')
    plt.title('Number of False Positive  from 2019-12 to 2023-02', fontsize=30)
    plt.tight_layout()
    plt.savefig(path_saving_file+'/n_fp_v3.png', dpi=350)

    perc_fp = []
    for i in range(len(fp_list)):
        if n_list[i] != 0:
            perc_fp.append(round(fp_list[i] / n_list[i], 2))
        else:
            perc_fp.append(0)
    x = np.arange(len(fp_list))
    fig, ax = plt.subplots(figsize=(32, 14))
    plt.rcParams.update({'font.size': 18})
    ax.spines['right'].set_visible(False)
    ax.spines['top'].set_visible(False)
    r = plt.bar(x, perc_fp, width=0.35, alpha=0.8, color='#a32b15')
    #plt.bar_label(r, rotation=0, fontsize=16)
    plt.yticks(fontsize=25)
    plt.xticks(x, labels=[str(y + 2) for y in x], rotation=45, fontsize=20)
    plt.xlabel('Week', fontsize=25)

    plt.title('Percentage of False Positive from 2019-12 to 2023-02', fontsize=25)
    plt.savefig(path_saving_file+'/perc_fp.png', dpi=350)

    # Calcolo flag
    nseq_week = Compute_nseq_week(prediction_lineages)
    type_of_lineage = list(nseq_week[:, 0])
    week_seq = list([int(x) for x in nseq_week[:, 1]])
    plt.figure(1)
    plt.bar(type_of_lineage, week_seq)
    plt.xlabel("Variant")
    plt.ylabel("No. of weeks")
    plt.title("Weeks for search 100 type of variant")
    plt.savefig(path_saving_file + '/100_variant.png', dpi=350)

    # Confusion Matrix
    A = sensitivity(summary_lineages,
                    path_saving_file)  # create the Confusion Matrix
    print('Compute the confusion matrix')

    # Distance
    Week_before=discovery(summary_lineages) # Weeks before our model identified a FDL as an anomaly
    Week_before_np=np.array(Week_before)
    print('-----------------------------------------week_before_FDLs-------------------------------------------------------')
    print(Week_before_np)
    print('---------------------------------------------------------------------------------------------------------------------------')

if __name__ == "__main__":
    parser = OptionParser()

    parser.add_option("-p", "--pathdrive", dest="path_drive",

                      help="path to drive example: path/folder/", default="")   # default
    parser.add_option("-c", "--csv", dest="csv_path",

                      help="path to CSV file metadata", default="")

    parser.add_option("-k","--kmers",dest="kmers",
                      help="path of file kmers",default='')

    parser.add_option("-s", "--pathsave ", dest="path_save",
                      help="path where we can save the file", default='')

    parser.add_option("-m", "--mantain ", dest="rate_mantain",
                      help="rate for mantain the k-mers", default=0.05)

    parser.add_option("-e", "--Epoch ", dest="number_epoch",
                      help="number of epochs", default=10)

    parser.add_option("-b", "--Batchsize ", dest="batch_size",
                      help="number of batchsize in the first week", default=256)

    parser.add_option("-d", "--encoding dimension ", dest="encoding_dim",
                      help="encodin dimention", default=1024)

    parser.add_option("-r", "--reduction facor ", dest="red_factor",
                      help="red_factor", default=1e-7)

    (options, args) = parser.parse_args()
    main(options)
